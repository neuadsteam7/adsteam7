{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from numpy import argmax\n",
    "from boruta import BorutaPy\n",
    "from tpot import TPOTRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', Cleaner()), ('normalizer', Normalizer()), ('train_test_split', SplitData()), ('features', FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('Boruta_GB', BorutaPy(alpha=0.05,\n",
       "     estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learni...timators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame.from_csv(\"https://raw.githubusercontent.com/LuisM78/Appliances-energy-prediction-data/master/energydata_complete.csv\")\n",
    "\n",
    "class Cleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe, performs cleaning if needed and returns cleaned dataframe\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def seconds(self, x):\n",
    "        sec = x.hour*3600+x.minute*60+x.second\n",
    "        return sec\n",
    "    \n",
    "    def day_week(self, z):\n",
    "        a=[]\n",
    "        for y in z:\n",
    "            if y == 0:\n",
    "                a.append('Monday')\n",
    "            elif y == 1:\n",
    "                a.append('Tuesday')\n",
    "            elif y == 2:\n",
    "                a.append('Wednesday')\n",
    "            elif y == 3:\n",
    "                a.append('Thrusday')\n",
    "            elif y == 4:\n",
    "                a.append('Friday')\n",
    "            elif y == 5:\n",
    "                a.append('Saturday')\n",
    "            elif y == 6:\n",
    "                a.append('Sunday')\n",
    "        return a\n",
    "    \n",
    "    def week(self, x):\n",
    "        a=[]\n",
    "        for y in x:\n",
    "            if y == 'Saturday' or y == 'Sunday':\n",
    "                a.append('weekend')\n",
    "            else:\n",
    "                a.append('weekday')\n",
    "        return a\n",
    "    \n",
    "    def one_hot_encode(self, Data):\n",
    "        label_encoder = LabelEncoder()\n",
    "        int_encoded = label_encoder.fit_transform(Data['week_status'])\n",
    "        int_encoded_day = label_encoder.fit_transform(Data['Day_Status'])\n",
    "        onehot_encoder = OneHotEncoder(sparse=False)\n",
    "        int_encoded = int_encoded.reshape(len(int_encoded), 1)\n",
    "        int_encoded_day = int_encoded_day.reshape(len(int_encoded_day), 1)\n",
    "        newWeek = onehot_encoder.fit_transform(int_encoded)\n",
    "        newDay = onehot_encoder.fit_transform(int_encoded_day)\n",
    "        # new2 = label_encoder.inverse_transform([argmax(new[len(new)-1, :])])\n",
    "        Data.drop(['week_status', 'Day_Status'], axis=1, inplace=True)\n",
    "        Data['Friday'] = pd.Series(newDay[:,0], index=Data.index)\n",
    "        Data['Monday'] = pd.Series(newDay[:,1], index=Data.index)\n",
    "        Data['Saturday'] = pd.Series(newDay[:,2], index=Data.index)\n",
    "        Data['Sunday'] = pd.Series(newDay[:,3], index=Data.index)\n",
    "        Data['Thursday'] = pd.Series(newDay[:,4], index=Data.index)\n",
    "        Data['Tuesday'] = pd.Series(newDay[:,5], index=Data.index)\n",
    "        Data['Wednesday'] = pd.Series(newDay[:,6], index=Data.index)\n",
    "        Data['WeekDay'] = pd.Series(newWeek[:,0], index=Data.index)\n",
    "        Data['Weekend'] = pd.Series(newWeek[:,1], index=Data.index)\n",
    "        return Data\n",
    "\n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"Adding the columns Day_Status, week_status and Num_sec_midnight\"\"\"\n",
    "        \n",
    "        df['Num_sec_midnight']=self.seconds(df.index)\n",
    "        z = df.index.dayofweek\n",
    "        df['Day_Status'] = z\n",
    "        df['Day_Status'] = self.day_week(df.Day_Status)\n",
    "        df['week_status'] = self.week(df.Day_Status)\n",
    "        \n",
    "        \"\"\"Performing one hot encoding on week_status and day_status columns\"\"\"\n",
    "        df=self.one_hot_encode(df)\n",
    "        return df\n",
    "\n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "\n",
    "class Normalizer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        \"\"\"Performs Normalization on all the columns except for Appliances\"\"\"\n",
    "        for j in range(1, len(df.columns)-1,1):\n",
    "            df.iloc[:,[j]] = (df.iloc[:,[j]] - df.iloc[:,[j]].mean())/df.iloc[:,[j]].std()\n",
    "        df.to_csv(\"normalized.csv\")\n",
    "        return df\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class SplitData(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df, y=None):\n",
    "        y = df['Appliances']\n",
    "        df4 = df.iloc[:,1:]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(df4, y, test_size=0.25)\n",
    "        train = X_train.join(y_train)\n",
    "        test = X_test.join(y_test)\n",
    "        train.to_csv(\"train.csv\")\n",
    "        test.to_csv(\"test.csv\")\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "pipeline = Pipeline([(\"cleaner\", Cleaner()),\n",
    "                     (\"normalizer\", Normalizer()),\n",
    "                     (\"train_test_split\", SplitData()),\n",
    "                     (\"features\", FeatureUnion([\n",
    "                         (\"Boruta_GB\", BorutaPy(GradientBoostingRegressor())),\n",
    "                         (\"Boruta_rf\", BorutaPy(RandomForestRegressor())),\n",
    "                         (\"Boruta_TP\", BorutaPy(TPOTRegressor())),\n",
    "                         (\"Boruta_ML\", BorutaPy(MLPRegressor(hidden_layer_sizes=(50,50,50,50,50,50), max_iter=1000)))\n",
    "                     ])),\n",
    "                     (\"estimator\", RandomForestRegressor())\n",
    "                    ])\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cleaner', Cleaner()),\n",
       " ('normalizer', Normalizer()),\n",
       " ('train_test_split', SplitData()),\n",
       " ('features', FeatureUnion(n_jobs=1,\n",
       "         transformer_list=[('Boruta_GB', BorutaPy(alpha=0.05,\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "               learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "               max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "               min_impur...\n",
       "       max_iter=100, n_estimators=1000, perc=100, random_state=None,\n",
       "       two_step=True, verbose=0))],\n",
       "         transformer_weights=None)),\n",
       " ('estimator',\n",
       "  RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "             max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Cleaner().transform(dataset)\n",
    "dataset = Normalizer().transform(dataset)\n",
    "X_train, X_test, y_train, y_test = SplitData().transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 258 candidates, totalling 2580 fits\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'tpot.operator_utils.TPOT_FeatureAgglomeration'>: attribute lookup TPOT_FeatureAgglomeration on tpot.operator_utils failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bd8d116249dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m              'Boruta_ML__mutation_rate':[0.5,0.7,0.9], 'Boruta_ML__crossover_rate':[0.3,0.5,0.7]}]\n\u001b[0;32m      6\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[1;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[0;32m    422\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m                         \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\pool.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m                 \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m                 \u001b[0mCustomizablePickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reducers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <class 'tpot.operator_utils.TPOT_FeatureAgglomeration'>: attribute lookup TPOT_FeatureAgglomeration on tpot.operator_utils failed"
     ]
    }
   ],
   "source": [
    "param_grid = [{'Boruta_GB__n_estimators': [500,1000,1500], 'Boruta_GB__max_depth':[4,5,6]},\n",
    "            {'Boruta_rf__n_estimators': [100,200,300]},\n",
    "             {'Boruta_ML__max_iter':[500,1000,1500]},\n",
    "             {'Boruta_TP__generations':[100,200,300], 'Boruta_ML__population_size':[100,300,500], 'Boruta_ML__offspring_size':[40,50,60],\n",
    "             'Boruta_ML__mutation_rate':[0.5,0.7,0.9], 'Boruta_ML__crossover_rate':[0.3,0.5,0.7]}]\n",
    "grid = GridSearchCV(pipeline, cv = 10, param_grid=param_grid, n_jobs=2,verbose=2)\n",
    "grid.fit(X_train, y_train)\n",
    "pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
